{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuL99n1VGs8P"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Nils Knieling\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzZv3J78QCS_"
      },
      "source": [
        "# Building AI-powered applications using LangChain and Google Vertex AI\n",
        "\n",
        "[![Open in Colab](https://img.shields.io/badge/Open%20in%20Colab-%23F9AB00.svg?logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/Cyclenerd/toolbox/blob/master/notebooks/LangChain_VertexAI.ipynb)\n",
        "[![Open in Vertex AI Workbench](https://img.shields.io/badge/Open%20in%20Vertex%20AI%20Workbench-%234285F4.svg?logo=googlecloud&logoColor=white)](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/Cyclenerd/toolbox/master/notebooks/LangChain_VertexAI.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View%20on%20GitHub-181717.svg?logo=github&logoColor=white)](https://github.com/Cyclenerd/toolbox/blob/master/notebooks/LangChain_VertexAI.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ9EgilGPrqb"
      },
      "source": [
        "## Install required packages\n",
        "\n",
        ">⚠️ You may receive a warning to \"Restart Runtime\" after the packages are installed. Don't worry, the subsequent cells will help you restart the runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RGrwGPNUMfK_"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Install dependencies\n",
        "\n",
        "!pip install langchain==0.0.244\n",
        "!pip install -U google-cloud-aiplatform==1.28.1 \"shapely < 2.0.0\"\n",
        "!pip install google-cloud-storage==2.8.0\n",
        "!pip install unstructured==0.8.4\n",
        "!pip install faiss-cpu==1.7.4\n",
        "!pip install chromadb==0.4.3\n",
        "\n",
        "print(\"☑️ Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "thYKV63LOzel"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Restart\n",
        "\n",
        "# Automatically restart kernel after installs so that your environment\n",
        "# can access the new packages.\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtJX8Z6UP5dJ"
      },
      "source": [
        "## Setup Google Cloud environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M9YVe5-xOfye"
      },
      "outputs": [],
      "source": [
        "# @markdown ✏️ Replace the placeholder text below:\n",
        "\n",
        "# Please fill in these values.\n",
        "project_id = \"test-nils-ai\"  # @param {type:\"string\"}\n",
        "region = \"us-central1\"  # @param {type:\"string\"}\n",
        "bucket = \"test-nils-data-man\"  # @param {type:\"string\"}\n",
        "\n",
        "# Quick input validations.\n",
        "assert project_id, \"⚠️ Please provide a Google Cloud project ID\"\n",
        "assert region, \"⚠️ Please provide a Google Cloud region\"\n",
        "assert bucket, \"⚠️ Please provide a Google Cloud storage bucket\"\n",
        "\n",
        "# Configure gcloud.\n",
        "!gcloud config set project \"{project_id}\"\n",
        "!gcloud config set storage/parallel_composite_upload_enabled \"True\"\n",
        "\n",
        "print(\"☑️ Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YPTem9iQOpxm"
      },
      "outputs": [],
      "source": [
        "#@markdown ### (Colab only!) Authenticate your Google Cloud Account\n",
        "\n",
        "# Authenticate gcloud.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rVzL_29f9gKF"
      },
      "outputs": [],
      "source": [
        "#@markdown ###  Check authenticated user\n",
        "current_user = !gcloud auth list \\\n",
        "  --filter=\"status:ACTIVE\" \\\n",
        "  --format=\"value(account)\" \\\n",
        "  --quiet\n",
        "\n",
        "current_user = current_user[0]\n",
        "print(f\"Current user: {current_user}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WQgrj8Hh9kIa"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Enable APIs\n",
        "\n",
        "# Enable APIs\n",
        "my_google_apis = [\n",
        "    \"storage.googleapis.com\",\n",
        "    \"aiplatform.googleapis.com\",\n",
        "]\n",
        "\n",
        "for api in my_google_apis :\n",
        "  print(f\"Enable API: {api}\")\n",
        "  !gcloud services enable \"{api}\" \\\n",
        "    --project=\"{project_id}\" \\\n",
        "    --quiet\n",
        "\n",
        "print(\"☑️ OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4XZc6aME9up1"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Create storage bucket for data\n",
        "\n",
        "#@markdown > Only necessary if the bucket does not already exist!\n",
        "!gcloud storage buckets create 'gs://{bucket}' \\\n",
        "  --location='{region}' \\\n",
        "  --uniform-bucket-level-access \\\n",
        "  --quiet\n",
        "\n",
        "print(\"☑️ Done\")\n",
        "print(f\"Open in console: https://console.cloud.google.com/storage/browser/{bucket}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrb66nGFQupe"
      },
      "source": [
        "## LangChain & Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKnQ4iMkMhvY"
      },
      "outputs": [],
      "source": [
        "#@markdown #### Import and print versions\n",
        "\n",
        "import sys\n",
        "print(f\"☑️ Python: {sys.version}\")\n",
        "\n",
        "# Langchain\n",
        "import langchain\n",
        "\n",
        "print(f\"☑️ LangChain version: {langchain.__version__}\")\n",
        "\n",
        "# Vertex AI\n",
        "# https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm\n",
        "from google.cloud import aiplatform, aiplatform_v1beta1\n",
        "from langchain.llms import VertexAI\n",
        "\n",
        "aiplatform.init(\n",
        "    project=project_id,\n",
        "    location=region,\n",
        ")\n",
        "\n",
        "print(f\"☑️ Vertex AI SDK version: {aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2IAMOVj-KxI"
      },
      "source": [
        "## Staging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyRgyd_AzMtk"
      },
      "source": [
        "### Directory Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmp_tT1Eawlg"
      },
      "outputs": [],
      "source": [
        "# Load documents from bucket.\n",
        "# This code snippet may run for a few (>5) minutes.\n",
        "\n",
        "# https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory\n",
        "from langchain.document_loaders import GCSDirectoryLoader\n",
        "\n",
        "loader = GCSDirectoryLoader(project_name=f\"{project_id}\", bucket=f\"{bucket}\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"☑️ You have {len(documents)} documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdxlY5V_vIXD"
      },
      "source": [
        "### Text Splitter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp8K_cUrdQEL"
      },
      "outputs": [],
      "source": [
        "# Split long text descriptions into smaller chunks that can fit into\n",
        "# the API request size limit, as expected by the LLM providers.\n",
        "\n",
        "# https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\".\", \"\\n\"],\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "chunked = text_splitter.split_documents(documents)\n",
        "\n",
        "print(\"Preview:\")\n",
        "print(chunked[0].page_content, \"\\n\")\n",
        "print(chunked[1].page_content)\n",
        "\n",
        "print(\"☑️ Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOU-nXszvCB3"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E3E_9fCqfgAX"
      },
      "outputs": [],
      "source": [
        "# Generate the vector embeddings for each chunk of text.\n",
        "# This code snippet may run for a few (>28) minutes.\n",
        "\n",
        "# Utils\n",
        "import time\n",
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# https://python.langchain.com/docs/integrations/text_embedding/google_vertex_ai_palm\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "\n",
        "embeddings_service = VertexAIEmbeddings()\n",
        "\n",
        "# Facebook AI Similarity Search (FAISS), local Vector Store\n",
        "# https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/\n",
        "from langchain.vectorstores import FAISS\n",
        "# TODO: Use Vertex AI Matching Engine as Vector Store.\n",
        "#       https://cloud.google.com/vertex-ai/docs/matching-engine/overview\n",
        "#       https://python.langchain.com/docs/integrations/vectorstores/matchingengine\n",
        "\n",
        "# Alternative...\n",
        "# Chroma, the local AI-native open-source embedding database\n",
        "# https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "\n",
        "# Utility functions for Embeddings API with rate limiting\n",
        "def rate_limit(max_per_minute):\n",
        "    period = 60 / max_per_minute\n",
        "    print(\"Waiting\")\n",
        "    while True:\n",
        "        before = time.time()\n",
        "        yield\n",
        "        after = time.time()\n",
        "        elapsed = after - before\n",
        "        sleep_time = max(0, period - elapsed)\n",
        "        if sleep_time > 0:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
        "    requests_per_minute: int\n",
        "    num_instances_per_batch: int\n",
        "\n",
        "    # Overriding embed_documents method\n",
        "    def embed_documents(self, texts: List[str]):\n",
        "        limiter = rate_limit(self.requests_per_minute)\n",
        "        results = []\n",
        "        docs = list(texts)\n",
        "\n",
        "        while docs:\n",
        "            # Working in batches because the Vertex AI API accepts maximum 5\n",
        "            # documents per request to get embeddings\n",
        "            head, docs = (\n",
        "                docs[: self.num_instances_per_batch],\n",
        "                docs[self.num_instances_per_batch :],\n",
        "            )\n",
        "            chunk = self.client.get_embeddings(head)\n",
        "            results.extend(chunk)\n",
        "            next(limiter)\n",
        "\n",
        "        return [r.values for r in results]\n",
        "\n",
        "\n",
        "# Embedding\n",
        "EMBEDDING_QPM = 100\n",
        "EMBEDDING_NUM_BATCH = 5\n",
        "embeddings = CustomVertexAIEmbeddings(\n",
        "    requests_per_minute=EMBEDDING_QPM,\n",
        "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
        ")\n",
        "\n",
        "# Embed your texts\n",
        "# FAISS...\n",
        "#db = FAISS.from_documents(chunked, embeddings)\n",
        "# or Chroma...\n",
        "db = Chroma.from_documents(chunked, embeddings, persist_directory=\"./chroma_db\")\n",
        "# TODO: Load only from disk\n",
        "# db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
        "\n",
        "print(\"\\n☑️ Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOsYVrXkvldJ"
      },
      "outputs": [],
      "source": [
        "# Expose index to the retriever\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFIa2n_mlJw9"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "docs = retriever.get_relevant_documents(\n",
        "    \"Command to show uptime.\"\n",
        ")\n",
        "\n",
        "print(\"\\n\\n\".join([x.page_content[:400] for x in docs[:2]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y--PVmPXQz63"
      },
      "source": [
        "## Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TRO-wFrhvrlG"
      },
      "outputs": [],
      "source": [
        "# @markdown Enter search query in a simple English text.\n",
        "user_query = \"What is the default shell and the command to show the uptime?\"  # @param {type:\"string\"}\n",
        "\n",
        "# Quick input validations.\n",
        "assert user_query, \"⚠️ Please input a valid input search text\"\n",
        "\n",
        "# Create chain to answer questions\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Custom promt template\n",
        "# https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"I want you to act as a Linux expert.\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Make sure the answer is correct and don't output false content.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "# LLM model\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Uses LLM to synthesize results from the search index.\n",
        "# We use Vertex PaLM Text API for LLM\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs=chain_type_kwargs,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "result = qa({\"query\": user_query})\n",
        "\n",
        "print(\"Result: \")\n",
        "print(result[\"result\"])\n",
        "\n",
        "print(\"Sources: \")\n",
        "print(result[\"source_documents\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
